metadata:
  version: "1.0"
  author: "ER"
  description: "This notebook evaluates different LLM models across various benchmark types such as MMLU, FinanceQA, and Contextual QA."
  supported_models:
    - llama3.1
    - llama3.1_az
  benchmark_types:
    QA: "Handles questions with simple Q&A format"
    Arzuman: "Handles questions with options where one is correct"
    ContextQA: "Handles questions with context and answers"
    Reshad: "Handles questions with topic-based options where one is correct"
  dataset_naming_convention:
    _mmlu_fqa: "Arzuman"
    _cqa: "ContextQA"
    _qa: "QA"
    _tc: "Reshad"

dataset_files:
  - "datasets/input_datasets/LLM_BENCH_qa.xlsx"
  # - "datasets/input_datasets/Quad_benchmark_cqa.xlsx"
  # - "datasets/input_datasets/banking-benchmark-405b-vs-8b_mmlu_fqa.xlsx"
  # - "datasets/input_datasets/LLM-Benchmark-reshad_tc.xlsx",

output:
  results_file: "datasets/output_datasets/benchmark_results_yaml.xlsx"
